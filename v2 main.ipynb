{"cells":[{"cell_type":"markdown","metadata":{},"source":["KUL H02A5a Computer Vision: Group Assignment 2\n","---------------------------------------------------------------\n","Student numbers: <span style=\"color:red\">r0810913, r0792692, r0795760, r0795985, r0799260</span>."]},{"cell_type":"markdown","metadata":{},"source":["# 1. Overview\n","This assignment consists of *three main parts*:\n","* Image classification (Sect. 2)\n","* Semantic segmentation (Sect. 3)\n","* Adversarial attacks (Sect. 4)\n","\n","In the first part, an end-to-end neural network is trained for image classification. In the second part, the same is done for semantic segmentation. In the third part, we try to find and exploit the weaknesses of our classification and/or segmentation network. Finally, a reflection and overall discussion with links to the lectures and \"real world\" computer vision is provided in section (Sect. 5)."]},{"cell_type":"markdown","metadata":{},"source":["## 1.1 Deep learning resources"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2024-05-21T08:35:52.513486Z","iopub.status.busy":"2024-05-21T08:35:52.513035Z","iopub.status.idle":"2024-05-21T08:35:52.522733Z","shell.execute_reply":"2024-05-21T08:35:52.521728Z","shell.execute_reply.started":"2024-05-21T08:35:52.513447Z"},"trusted":true},"outputs":[],"source":["import numpy as np\n","import pandas as pd\n","import tensorflow as tf\n","from matplotlib import pyplot as plt\n","import os\n","import cv2\n","from urllib import request\n","from sklearn.model_selection import train_test_split\n","import tensorflow as tf\n","from tensorflow.keras.preprocessing.image import ImageDataGenerator\n","from tensorflow.keras import Model\n","from tensorflow.keras.models import Sequential\n","from tensorflow.keras.utils import to_categorical\n","from tensorflow.keras.losses import categorical_crossentropy\n","from tensorflow.keras.preprocessing.image import ImageDataGenerator\n","from tensorflow.keras.layers import Dense, Flatten, Conv2D, MaxPooling2D, Dropout, Input, Add, ActivityRegularization\n","from tensorflow.keras import models, layers\n","from tensorflow.keras.optimizers import Adadelta\n","from tensorflow.keras.metrics import Precision, Recall\n","from keras import regularizers\n","from keras.layers import BatchNormalization\n","from sklearn.metrics import f1_score\n","from keras.callbacks import ModelCheckpoint, EarlyStopping\n","from skimage.transform import resize\n","from tensorflow.keras.optimizers import Adam\n","from IPython.display import clear_output"]},{"cell_type":"markdown","metadata":{},"source":["## 1.2 PASCAL VOC 2009\n","For this project we will be using the [PASCAL VOC 2009](http://host.robots.ox.ac.uk/pascal/VOC/voc2009/index.html) dataset. This dataset consists of colour images of various scenes with different object classes (e.g. animal: *bird, cat, ...*; vehicle: *aeroplane, bicycle, ...*), totalling 20 classes."]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2024-05-21T08:35:57.820112Z","iopub.status.busy":"2024-05-21T08:35:57.819420Z","iopub.status.idle":"2024-05-21T08:36:14.075916Z","shell.execute_reply":"2024-05-21T08:36:14.074928Z","shell.execute_reply.started":"2024-05-21T08:35:57.820079Z"},"trusted":true},"outputs":[],"source":["# Loading the training data\n","train_df = pd.read_csv('//kaggle/input/kul-h02a5a-computer-vision-ga2-2024/train/train_set.csv', index_col=\"Id\")\n","labels = train_df.columns\n","train_df[\"img\"] = [np.load('/kaggle/input/kul-h02a5a-computer-vision-ga2-2024/train/img/train_{}.npy'.format(idx)) for idx, _ in train_df.iterrows()]\n","train_df[\"seg\"] = [np.load('/kaggle/input/kul-h02a5a-computer-vision-ga2-2024/train/seg/train_{}.npy'.format(idx)) for idx, _ in train_df.iterrows()]\n","print(\"The training set contains {} examples.\".format(len(train_df)))\n","\n","# Show some examples\n","fig, axs = plt.subplots(2, 20, figsize=(10 * 20, 10 * 2))\n","for i, label in enumerate(labels):\n","    df = train_df.loc[train_df[label] == 1]\n","    axs[0, i].imshow(df.iloc[0][\"img\"], vmin=0, vmax=255)\n","    axs[0, i].set_title(\"\\n\".join(label for label in labels if df.iloc[0][label] == 1), fontsize=40)\n","    axs[0, i].axis(\"off\")\n","    axs[1, i].imshow(df.iloc[0][\"seg\"], vmin=0, vmax=20)  # with the absolute color scale it will be clear that the arrays in the \"seg\" column are label maps (labels in [0, 20])\n","    axs[1, i].axis(\"off\")\n","    \n","plt.show()\n","\n","# The training dataframe contains for each image 20 columns with the ground truth classification labels and 20 column with the ground truth segmentation maps for each class\n","train_df.head(1)\n","\n","# Loading the test data\n","test_df = pd.read_csv('/kaggle/input/kul-h02a5a-computer-vision-ga2-2024/test/test_set.csv', index_col=\"Id\")\n","test_df[\"img\"] = [np.load('/kaggle/input/kul-h02a5a-computer-vision-ga2-2024/test/img/test_{}.npy'.format(idx)) for idx, _ in test_df.iterrows()]\n","test_df[\"seg\"] = [-1 * np.ones(img.shape[:2], dtype=np.int8) for img in test_df[\"img\"]]\n","print(\"The test set contains {} examples.\".format(len(test_df)))\n","\n","# The test dataframe is similar to the training dataframe, but here the values are -1 --> your task is to fill in these as good as possible in Sect. 2 and Sect. 3; in Sect. 6 this dataframe is automatically transformed in the submission CSV!\n","test_df.head(1)"]},{"cell_type":"markdown","metadata":{},"source":["## 1.3 Kaggle submission\n","The filled test dataframe (during Sect. 2 and Sect. 3) must be converted to a submission.csv with two rows per example (one for classification and one for segmentation) and with only a single prediction column (the multi-class/label predictions running length encoded). The following functions are used to generate this submission file."]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2024-05-21T08:36:19.641916Z","iopub.status.busy":"2024-05-21T08:36:19.641209Z","iopub.status.idle":"2024-05-21T08:36:19.651723Z","shell.execute_reply":"2024-05-21T08:36:19.650724Z","shell.execute_reply.started":"2024-05-21T08:36:19.641886Z"},"trusted":true},"outputs":[],"source":["def _rle_encode(img):\n","    \"\"\"\n","    Kaggle requires RLE encoded predictions for computation of the Dice score (https://www.kaggle.com/lifa08/run-length-encode-and-decode)\n","\n","    Parameters\n","    ----------\n","    img: np.ndarray - binary img array\n","    \n","    Returns\n","    -------\n","    rle: String - running length encoded version of img\n","    \"\"\"\n","    pixels = img.flatten()\n","    pixels = np.concatenate([[0], pixels, [0]])\n","    runs = np.where(pixels[1:] != pixels[:-1])[0] + 1\n","    runs[1::2] -= runs[::2]\n","    rle = ' '.join(str(x) for x in runs)\n","    return rle\n","\n","def generate_submission(df):\n","    \"\"\"\n","    Make sure to call this function once after you completed Sect. 2 and Sect. 3! It transforms and writes your test dataframe into a submission.csv file.\n","    \n","    Parameters\n","    ----------\n","    df: pd.DataFrame - filled dataframe that needs to be converted\n","    \n","    Returns\n","    -------\n","    submission_df: pd.DataFrame - df in submission format.\n","    \"\"\"\n","    df_dict = {\"Id\": [], \"Predicted\": []}\n","    for idx, _ in df.iterrows():\n","        df_dict[\"Id\"].append(f\"{idx}_classification\")\n","        df_dict[\"Predicted\"].append(_rle_encode(np.array(df.loc[idx, labels])))\n","        df_dict[\"Id\"].append(f\"{idx}_segmentation\")\n","        df_dict[\"Predicted\"].append(_rle_encode(np.array([df.loc[idx, \"seg\"] == j + 1 for j in range(len(labels))])))\n","    \n","    submission_df = pd.DataFrame(data=df_dict, dtype=str).set_index(\"Id\")\n","    submission_df.to_csv(\"submission.csv\")\n","    return submission_df"]},{"cell_type":"markdown","metadata":{},"source":["## 1.4 Preprocessing/General functions\n","\n","This section contains general purpose functions that are used for example to preprocess \n","the data sets before neural networks can be trained with them.\n","\n","The training data undergoes the following preprocessing steps:\n","* Resizing of the images to 224 x 224.\n","* Data augmentation is applied to increase the number of data samples. New samples are generated by rotating, flipping, and adjusting the brightness of the existing training data. These augmented samples are then added to the dataset, enhancing the number of training samples and improving generalization.\n","\n","The test data is only resized.\n"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2024-05-21T08:51:40.520273Z","iopub.status.busy":"2024-05-21T08:51:40.519887Z","iopub.status.idle":"2024-05-21T08:51:40.533236Z","shell.execute_reply":"2024-05-21T08:51:40.532262Z","shell.execute_reply.started":"2024-05-21T08:51:40.520242Z"},"trusted":true},"outputs":[],"source":["def resize_image(img, height, width):\n","    img_size = (height,width,3)\n","#     normal_image = img.astype(np.float32)/ 255.0\n","    resized = cv2.resize(img, (width, height), interpolation=cv2.INTER_CUBIC)\n","    return resized\n","\n","\n","def preprocess_data(X, y, shape, augment=True):\n","    \"\"\"Resize and optionally augment the dataset\n","    In order to deal with class imbalance we only \n","    augment the images that do not contain people.\n","    \"\"\"\n","    height = shape[0]\n","    width = shape[1]\n","    X_res = []\n","    for i, img in enumerate(X):\n","        X_res.append(resize_image(img, height, width))\n","\n","    X_res = np.array(X_res)\n","\n","    if augment:\n","        to_augment = []\n","        to_augment_y = []\n","        for i, img in enumerate(X_res):\n","            to_augment.append(img)\n","            to_augment_y.append(y[i])\n","\n","\n","        to_augment = np.array(to_augment)\n","        to_augment_y = np.array(to_augment_y)\n","    \n","        datagen = ImageDataGenerator(\n","                    rotation_range=20,\n","                    brightness_range=[0.3,1.0],\n","                    horizontal_flip=True)\n","\n","        d = datagen.flow(to_augment, to_augment_y, batch_size=50, shuffle=False, seed=None)\n","        it = 0\n","\n","        new = []\n","        new_y = []\n","        for batch in d:\n","            it += 1\n","            for i, img in enumerate(batch[0]):\n","                new.append(img)\n","                new_y.append(batch[1][i])\n","            if it >= 50:\n","                break\n","\n","        new = np.array(new)\n","        new_y = np.array(new_y)\n","        \n","        data = np.concatenate((new, to_augment))\n","        data_y = np.concatenate((new_y, to_augment_y))\n","    else:\n","        data = X_res\n","        data_y = y\n","\n","        \n","    return data, data_y\n"]},{"cell_type":"markdown","metadata":{},"source":["\n","The following functions will be used later to display the images with their predicted or actual labels and to show the training history of the model."]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2024-05-21T08:36:28.203439Z","iopub.status.busy":"2024-05-21T08:36:28.202561Z","iopub.status.idle":"2024-05-21T08:36:28.218011Z","shell.execute_reply":"2024-05-21T08:36:28.217201Z","shell.execute_reply.started":"2024-05-21T08:36:28.203404Z"},"trusted":true},"outputs":[],"source":["def show_image_with_label(data, labels):\n","\n","    CLASSES = {'aeroplane': 0, 'bicycle': 1, 'bird': 2, 'boat': 3, 'bottle': 4, 'bus': 5,\n","            'car': 6, 'cat': 7, 'chair': 8, 'cow': 9, 'diningtable': 10, 'dog': 11,\n","            'horse': 12, 'motorbike':13, 'person': 14, 'pottedplant': 15, 'sheep': 16,\n","            'sofa': 17, 'train': 18, 'tvmonitor': 19}\n","    \n","    # Randomly select 10 indices\n","    indices = np.random.choice(len(labels), 10, replace=False)\n","\n","    # Create a figure and axis objects\n","    fig, axes = plt.subplots(1, 10, figsize=(20, 2))  # Adjust figsize as needed\n","\n","    # Show the selected 10 random images with their corresponding class names\n","    for i, idx in enumerate(indices):\n","        axes[i].imshow(data[idx])\n","        \n","        # Get the class names for the labels with value 1\n","        class_names = [class_name for class_name, label in CLASSES.items() if labels[idx][label] == 1]\n","        \n","        # Set title as class names separated by commas\n","        axes[i].set_title(', '.join(class_names))\n","        \n","        axes[i].axis('off')\n","\n","    plt.show()\n","    return \n","\n","def plot_history(history):\n","    fig, axs = plt.subplots(1, 3, figsize=(15, 5))  # Adjusting the layout to fit 3 plots\n","\n","    if \"Precision\" in history.history:\n","        # Summarize history for precision\n","        axs[0].plot(history.history['Precision'])  # Make sure the key matches your history dictionary\n","        axs[0].plot(history.history['val_Precision'])\n","        axs[0].set_title('Model Precision')\n","        axs[0].set_ylabel('Precision')\n","        axs[0].set_xlabel('Epoch')\n","        axs[0].legend(['Train', 'Validation'], loc='upper left')\n","\n","    if \"loss\" in history.history:\n","        # Summarize history for loss\n","        axs[1].plot(history.history['loss'])\n","        axs[1].plot(history.history['val_loss'])\n","        axs[1].set_title('Model Loss')\n","        axs[1].set_ylabel('Loss')\n","        axs[1].set_xlabel('Epoch')\n","        axs[1].legend(['Train', 'Validation'], loc='upper left')\n","\n","    if \"Accuracy\" in history.history:\n","        # Summarize history for accuracy\n","        axs[2].plot(history.history['Accuracy'])  # Again, make sure the key matches your history dictionary\n","        axs[2].plot(history.history['val_Accuracy'])\n","        axs[2].set_title('Model Accuracy')\n","        axs[2].set_ylabel('Accuracy')\n","        axs[2].set_xlabel('Epoch')\n","        axs[2].legend(['Train', 'Validation'], loc='upper left')\n","\n","    plt.tight_layout()  # This helps to fit everything into the figure cleanly\n","    plt.show()\n","    return"]},{"cell_type":"markdown","metadata":{},"source":["# 2. Image classification\n","The goal here is simple: implement a classification CNN and train it to recognise all 20 classes (and/or background) using the training set and compete on the test set (by filling in the classification columns in the test dataframe). For the multilabel classification task, where each image can have multiple labels, multiple CNN architectures were tried. In what follows, we summarise the main findings and attempts."]},{"cell_type":"markdown","metadata":{},"source":["We first preprocess our training set. This ensures all images are equally sized, normalized and also augments the data set with rotated images or images with reduced brightness. The neural networks used during the transfer learning later have been trained on images of size 224x224, so we will restrict all images to this size."]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2024-05-21T08:51:46.991754Z","iopub.status.busy":"2024-05-21T08:51:46.991102Z","iopub.status.idle":"2024-05-21T08:52:22.070767Z","shell.execute_reply":"2024-05-21T08:52:22.069709Z","shell.execute_reply.started":"2024-05-21T08:51:46.991724Z"},"trusted":true},"outputs":[],"source":["train_images = train_df[\"img\"].to_numpy()\n","train_labels = train_df[labels].to_numpy()\n","\n","IMG_HEIGHT = 224\n","IMG_WIDTH = 224\n","IMG_SIZE = (IMG_HEIGHT,IMG_WIDTH)\n","\n","X, y_train = preprocess_data(train_images, train_labels, IMG_SIZE, augment=True)\n","\n","X_train = X.astype(np.float32)\n","\n","X, y_test = preprocess_data(test_df[\"img\"].to_numpy(), test_df[labels].to_numpy(), IMG_SIZE, augment=False)\n","\n","X_test = X.astype(np.float32)\n","\n","print(\"Training set:\", X_train.shape, y_train.shape)\n","print(\"Test set:\", X_test.shape, y_test.shape)"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2024-05-21T08:51:31.583500Z","iopub.status.busy":"2024-05-21T08:51:31.582889Z","iopub.status.idle":"2024-05-21T08:51:32.206287Z","shell.execute_reply":"2024-05-21T08:51:32.205384Z","shell.execute_reply.started":"2024-05-21T08:51:31.583465Z"},"trusted":true},"outputs":[],"source":["show_image_with_label(X_train,y_train)\n"]},{"cell_type":"markdown","metadata":{},"source":["Some images contain multiple objects. Let's see how many images contain more than 1 object."]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2024-05-21T08:45:17.534640Z","iopub.status.busy":"2024-05-21T08:45:17.533925Z","iopub.status.idle":"2024-05-21T08:45:17.545359Z","shell.execute_reply":"2024-05-21T08:45:17.544407Z","shell.execute_reply.started":"2024-05-21T08:45:17.534607Z"},"trusted":true},"outputs":[],"source":["def count_images_with_multiple_objects(labels, label_columns):\n","    \"\"\"\n","    Function to count the number of images containing more than one object.\n","\n","    Args:\n","    labels (pd.DataFrame): DataFrame containing the labels for each image.\n","    label_columns (list): List of columns to consider for counting objects.\n","\n","    Returns:\n","    int: The number of images containing more than one object.\n","    \"\"\"\n","    # Sum the number of objects (values of 1) in each row for the specified columns\n","    object_counts = labels[label_columns].sum(axis=1)\n","\n","    # Count how many rows have more than one object\n","    images_with_multiple_objects = (object_counts > 1).sum()\n","\n","    return images_with_multiple_objects\n","\n","\n","# Specify the label columns (excluding 'img' and 'seg')\n","label_columns = [\n","    'aeroplane', 'bicycle', 'bird', 'boat', 'bottle', 'bus', 'car', 'cat',\n","    'chair', 'cow', 'horse', 'motorbike', 'person', 'pottedplant', 'sheep',\n","    'sofa', 'train', 'tvmonitor'\n","]\n","\n","# Print the number of images in the train data\n","total_images = len(train_df)\n","print(f\"Total number of images in the training data: {total_images}\")\n","\n","# Count the number of images containing more than one object\n","num_images_with_multiple_objects = count_images_with_multiple_objects(train_df, label_columns)\n","print(f\"Number of images containing more than one object: {num_images_with_multiple_objects}\")\n"]},{"cell_type":"markdown","metadata":{},"source":["Let's also take a look at the most prevalent classes in the training set."]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2024-05-21T08:45:13.772662Z","iopub.status.busy":"2024-05-21T08:45:13.772297Z","iopub.status.idle":"2024-05-21T08:45:14.312836Z","shell.execute_reply":"2024-05-21T08:45:14.311885Z","shell.execute_reply.started":"2024-05-21T08:45:13.772634Z"},"trusted":true},"outputs":[],"source":["# Define the classes dictionary\n","CLASSES = ['aeroplane', 'bicycle', 'bird', 'boat', 'bottle', 'bus',\n","           'car', 'cat', 'chair', 'cow', 'diningtable', 'dog',\n","           'horse', 'motorbike', 'person', 'pottedplant', 'sheep',\n","           'sofa', 'train', 'tvmonitor']\n","\n","# Initialize the class counts dictionary\n","class_counts = {class_name: 0 for class_name in CLASSES}\n","\n","# Count the occurrences of each class label\n","for idx, row in train_df.iterrows():\n","    for class_name in CLASSES:\n","        if row[class_name] == 1:\n","            class_counts[class_name] += 1\n","\n","# Calculate relative class counts\n","total_images = len(train_df)\n","rel_class_counts_list = [class_counts[class_name] / total_images for class_name in CLASSES]\n","\n","# Plot a histogram of class frequencies\n","plt.figure(figsize=(12, 8))\n","plt.bar(range(len(CLASSES)), rel_class_counts_list, tick_label=CLASSES)\n","plt.xticks(rotation=90)\n","plt.ylabel('Proportion of Images')\n","plt.title('Class Frequencies in the Training Data')"]},{"cell_type":"markdown","metadata":{},"source":["We see that \"person\" is most present in the images (>25% of images). Other objects appear in very few of the images."]},{"cell_type":"markdown","metadata":{},"source":["## 2.1 Basic CNN Model"]},{"cell_type":"markdown","metadata":{},"source":["Below is an explanation of our architectural decisions for constructing a CNN model:\n","\n","**Convolutional Blocks (4)**:\n","- Each block contains a convolutional layer with increasing filter counts to capture increasingly complex features.\n","\n","**ReLU Activation**:\n","- Applied after each Conv2D layer to capture non-linear features and prevent the vanishing gradient problem.\n","\n","**MaxPooling2D**:\n","- Downsamples feature maps, retaining the most salient features by selecting the maximum value in each window.\n","\n","**Dropout**:\n","- Prevents overfitting by randomly dropping neurons during training to reduce reliance on specific neurons.\n","\n","**Fully Connected Layers**:\n","- Flatten layer transforms the feature maps into 1D arrays.\n","- Dense layers learn non-linear combinations of these features.\n","\n","**Sigmoid Activation**:\n","- Used in the output layer to produce probabilities for each class in a multi-label classification task.\n","\n","**Binary Cross-Entropy Loss**:\n","- Suitable for multi-label classification, treating each class probability independently."]},{"cell_type":"markdown","metadata":{},"source":["Our basic model consist of four convolutional layers, each followed by the relu activation function and a maxpooling layer. These layer extract features from the images. The end of the network is a fully connected or dense neural network. \n","This network performs classification based on the extracted features."]},{"cell_type":"markdown","metadata":{},"source":["The binary crossentropy loss function is used in combination with the sigmoid activation function in the output layer for multilabel classification."]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2024-05-20T19:04:53.346443Z","iopub.status.busy":"2024-05-20T19:04:53.345596Z","iopub.status.idle":"2024-05-20T19:04:53.573886Z","shell.execute_reply":"2024-05-20T19:04:53.572914Z","shell.execute_reply.started":"2024-05-20T19:04:53.346409Z"},"trusted":true},"outputs":[],"source":["def get_basic_model(input_shape, nb_classes):\n","\n","    input_shape = (input_shape[0], input_shape[1], 3)\n","\n","    model = Sequential(name=\"OurModel\")\n","\n","    # Convolutional layers\n","    model.add(Conv2D(16, (3, 3), activation='relu', input_shape=input_shape))\n","    model.add(MaxPooling2D((2, 2)))\n","\n","    model.add(Conv2D(32, (3, 3), activation='relu'))\n","    model.add(MaxPooling2D((2, 2)))\n","    model.add(Dropout(0.25))\n","\n","    model.add(Conv2D(64, (3, 3), activation='relu'))\n","    model.add(MaxPooling2D((2, 2)))\n","    model.add(Dropout(0.25))\n","\n","    model.add(Conv2D(128, (3, 3), activation='relu'))\n","    model.add(MaxPooling2D((2, 2)))\n","    model.add(Dropout(0.25))\n","\n","    # Flatten layer\n","    model.add(Flatten())\n","\n","    # Fully connected layers\n","    model.add(Dense(512, activation='relu'))\n","    model.add(Dropout(0.5))\n","\n","    # Output layer\n","    model.add(Dense(nb_classes, activation='sigmoid'))\n","\n","    return model\n","\n","def compile_our_model():\n","    input_shape = IMG_SIZE\n","    nb_classes = 20\n","\n","    model = get_basic_model(input_shape, nb_classes)\n","    model.compile(optimizer='adam', loss='binary_crossentropy', metrics=['Precision', 'Recall', 'Accuracy'])\n","    summary = model.summary()\n","    print(summary)\n","    return model\n","\n","# Compile the model\n","our_model = compile_our_model()"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2024-05-20T19:04:58.785248Z","iopub.status.busy":"2024-05-20T19:04:58.784858Z","iopub.status.idle":"2024-05-20T19:04:58.792316Z","shell.execute_reply":"2024-05-20T19:04:58.791321Z","shell.execute_reply.started":"2024-05-20T19:04:58.785217Z"},"trusted":true},"outputs":[],"source":["def get_callbacks(patience, monitor_metric='val_loss', checkpoint_path='best_model.keras'):\n","    \"\"\"\n","    Create ModelCheckpoint and EarlyStopping callbacks.\n","\n","    Args:\n","    patience (int): Number of epochs with no improvement after which training will be stopped.\n","    monitor_metric (str): Metric to monitor for EarlyStopping and ModelCheckpoint.\n","    checkpoint_path (str): Path to save the best model.\n","\n","    Returns:\n","    list: List of callbacks.\n","    \"\"\"\n","    checkpoint_callback = ModelCheckpoint(\n","        checkpoint_path,              # Path where the model will be saved\n","        monitor=monitor_metric,       # Metric to monitor\n","        save_best_only=True,          # Save only the best model\n","        mode='max' if 'Accuracy' in monitor_metric else 'min',  # Mode 'max' if monitoring accuracy, else 'min'\n","        verbose=1                     # Logs out when models are being saved\n","    )\n","\n","    early_stopping_callback = EarlyStopping(\n","        monitor=monitor_metric,       # Metric to monitor\n","        min_delta=0.005,              # Minimum change to qualify as an improvement\n","        patience=patience,            # Number of epochs with no improvement after which training will be stopped\n","        mode='min' if 'loss' in monitor_metric else 'max',  # Mode 'min' if monitoring loss, else 'max'\n","        verbose=1                     # Logs out when training is being stopped\n","    )\n","\n","    return [checkpoint_callback, early_stopping_callback]"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2024-05-20T19:05:37.522421Z","iopub.status.busy":"2024-05-20T19:05:37.521987Z","iopub.status.idle":"2024-05-20T19:07:20.693367Z","shell.execute_reply":"2024-05-20T19:07:20.692379Z","shell.execute_reply.started":"2024-05-20T19:05:37.522386Z"},"trusted":true},"outputs":[],"source":["# Assuming X_train and y_train are defined and preprocessed\n","# Define number of epochs\n","epochs = 50\n","# Get callbacks with patience based on the number of epochs\n","callbacks = get_callbacks(patience=epochs//5, monitor_metric='val_Accuracy')\n","\n","history_basic_model = our_model.fit(\n","    X_train,\n","    y_train,\n","    validation_split=0.2,\n","    epochs=epochs,\n","    batch_size=64,\n","    callbacks=callbacks  # Include the callbacks in the training process\n",")"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2024-05-20T19:08:30.407438Z","iopub.status.busy":"2024-05-20T19:08:30.407003Z","iopub.status.idle":"2024-05-20T19:08:31.383752Z","shell.execute_reply":"2024-05-20T19:08:31.382584Z","shell.execute_reply.started":"2024-05-20T19:08:30.407407Z"},"trusted":true},"outputs":[],"source":["plot_history(history_basic_model)"]},{"cell_type":"markdown","metadata":{},"source":["Achieving only an accuracy of around 65% on the validation set, we can see that the obatained accuracy of the model is not very high. Moreover, the validation accuracy quickly levels off after a few training epochs. The validation loss even starts to increase significantly, indicating overfitting.\n","\n","A second architecture was also tried, which added batch normalization layers and multiple convolutional layers per convolutional block. The results were similar to the previously chosen model, except that convergence was more stable and quicker."]},{"cell_type":"markdown","metadata":{},"source":["## 2.2 Known Model Architectures (Alexnet)"]},{"cell_type":"markdown","metadata":{},"source":["As a first step to a more performant model, this section tries the well-known Alexnet architecture to classify images.\n","\n","The architecture consists of the following layers:\n","* 5 convolutional layers for feature detection\n","* 3 maxpooling layers for reducing the dimension of feature maps\n","* 3 Fully connected layers to learn from the extracted features of the convolutional layers\n","* 1 flatten layer to transition from the convolutional layers to the fully connected layers\n","* The binary crossentropy loss function is used in combination with the sigmoid activation function in the output layer for multilabel classification.\n","Reference: Krizhevsky, A., Sutskever, I., & Hinton, G.E. (2012). ImageNet classification with deep convolutional neural networks. Communications of the ACM, 60, 84 - 90.\n","\n","Since the full Alexnet architecture contains more than 50 million parameters, this is too extensive to train from scratch and would overfit on the limited training data available. For this reason, a reduced version of the Alexnet architecture was trained from scratch. The reduced Alexnet model has the same structure, but with fewer filters for each convolutional layer, and fewer units for each dense layer. This reduces the number of trainable parameters to 8.6 million.\n","\n","In addition, batch normalisation was added after each convolutional and fully connected layer. Thanks to the normalization of the outputs of the previous layer, the learning is sped up, the noise in the model is reduced and a higher learning rate can be used.\n","\n","The model was trained for 100 epochs, using the Adam optimizer.  The resulting net attains a higher accuracy for both the training and validation sets. The validation accuracy has increased to ~70%.\n","\n","reference: Saxena, S. (2024, 20 februari). Introduction to Batch Normalization. Analytics Vidhya. https://www.analyticsvidhya.com/blog/2021/03/introduction-to-batch-normalization/"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2024-05-20T19:09:05.902988Z","iopub.status.busy":"2024-05-20T19:09:05.902032Z","iopub.status.idle":"2024-05-20T19:09:06.472177Z","shell.execute_reply":"2024-05-20T19:09:06.471030Z","shell.execute_reply.started":"2024-05-20T19:09:05.902953Z"},"trusted":true},"outputs":[],"source":["from keras import regularizers\n","from keras.layers import BatchNormalization\n","from keras.callbacks import ModelCheckpoint\n","\n","\n","def get_alexnet_model_reduced(input_shape, nb_classes):\n","    input_shape = (input_shape[0], input_shape[1],3)\n","    model = models.Sequential(name=\"Alexnet\")\n","    \n","    # Convolutional layers\n","    model.add(layers.Conv2D(filters=16, kernel_size=(11,11), input_shape=input_shape, strides=4, padding=\"valid\", kernel_initializer=\"he_normal\", use_bias=False))\n","    model.add(BatchNormalization())\n","    model.add(layers.Activation(\"relu\"))\n","    model.add(layers.MaxPool2D(pool_size=(3,3), strides=(2,2), padding=\"valid\"))\n","    \n","    model.add(layers.Conv2D(filters=32, kernel_size=(5,5), strides=1, padding=\"same\", kernel_initializer=\"he_normal\", use_bias=False))\n","    model.add(BatchNormalization())\n","    model.add(layers.Activation(\"relu\"))\n","    model.add(layers.MaxPool2D(pool_size=(3,3), strides=(2,2), padding=\"valid\"))\n","    \n","    model.add(layers.Conv2D(filters=64, kernel_size=(3,3), strides=1, padding=\"same\", kernel_initializer=\"he_normal\", use_bias=False))\n","    model.add(BatchNormalization())\n","    model.add(layers.Activation(\"relu\"))\n","    \n","    model.add(layers.Conv2D(filters=64, kernel_size=(3,3), strides=1, padding=\"same\", kernel_initializer=\"he_normal\", use_bias=False))\n","    model.add(BatchNormalization())\n","    model.add(layers.Activation(\"relu\"))\n","    \n","    model.add(layers.Conv2D(filters=64, kernel_size=(3,3), strides=1, padding=\"same\", kernel_initializer=\"he_normal\", use_bias=False))\n","    model.add(BatchNormalization())\n","    model.add(layers.Activation(\"relu\"))\n","    model.add(layers.MaxPool2D(pool_size=(3,3), strides=(2,2), padding=\"valid\"))\n","    \n","    # Flatten layer\n","    model.add(layers.Flatten())\n","    \n","    # Fully connected layers\n","    model.add(layers.Dense(units=2048))\n","    model.add(BatchNormalization())\n","    model.add(layers.Activation(\"relu\"))\n","    \n","    model.add(layers.Dense(units=2048))\n","    model.add(BatchNormalization())\n","    model.add(layers.Activation(\"relu\"))\n","    \n","    model.add(layers.Dense(units=500))\n","    model.add(BatchNormalization())\n","    model.add(layers.Activation(\"relu\"))\n","    \n","    # Output layer\n","    model.add(layers.Dense(units=nb_classes, activation=\"sigmoid\"))\n","    \n","    return model\n","\n","def compile_alexnet_model_reduced():\n","    alexnet_reduced=get_alexnet_model_reduced(IMG_SIZE,20)\n","    filepath = \"best_model.keras\"\n","\n","    # Define the checkpoint to monitor validation accuracy and save the best model\n","    checkpoint = ModelCheckpoint(filepath, monitor='val_accuracy', verbose=1, save_best_only=True, mode='max')\n","\n","    alexnet_reduced.compile(optimizer='adam', loss='binary_crossentropy', metrics=['Precision', 'Recall', 'Accuracy'])\n","    summary=alexnet_reduced.summary()\n","    print(summary)\n","    return alexnet_reduced\n","\n","alexnet_reduced=compile_alexnet_model_reduced()"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2024-05-20T19:11:54.788037Z","iopub.status.busy":"2024-05-20T19:11:54.787320Z","iopub.status.idle":"2024-05-20T19:15:56.662110Z","shell.execute_reply":"2024-05-20T19:15:56.661009Z","shell.execute_reply.started":"2024-05-20T19:11:54.788003Z"},"trusted":true},"outputs":[],"source":["checkpoint_callback = ModelCheckpoint(\n","    'alexnet_reduced_best.keras', # Path where the model will be saved\n","    monitor='val_Accuracy',       # Metric to monitor\n","    save_best_only=True,          # Save only the best model\n","    mode='max',                   # Mode 'max' because we aim to maximize the validation accuracy\n","    verbose=1                     # Logs out when models are being saved\n",")\n","\n","history_alexnet_reduced=alexnet_reduced.fit(X_train, y_train, epochs=100, batch_size=16, validation_split=0.2,callbacks=[checkpoint_callback])"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2024-05-20T19:19:10.030105Z","iopub.status.busy":"2024-05-20T19:19:10.029663Z","iopub.status.idle":"2024-05-20T19:19:11.273507Z","shell.execute_reply":"2024-05-20T19:19:11.272299Z","shell.execute_reply.started":"2024-05-20T19:19:10.030052Z"},"trusted":true},"outputs":[],"source":["plot_history(history_alexnet_reduced)"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["y_pred_prob  = alexnet_reduced.predict(X_test)\n","\n","threshold = 0.3\n","\n","y_pred = np.array(y_pred_prob>threshold, int)"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["show_image_with_label(X_test,y_pred)"]},{"cell_type":"markdown","metadata":{},"source":["By plotting some images of the test set, along with the label that the reduced Alexnet predicts,we can see that the model still is far from performing optimally. We continue to look for improvements, this time by applying transfer learning."]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2024-05-20T19:19:22.317593Z","iopub.status.busy":"2024-05-20T19:19:22.317200Z","iopub.status.idle":"2024-05-20T19:19:28.111059Z","shell.execute_reply":"2024-05-20T19:19:28.110145Z","shell.execute_reply.started":"2024-05-20T19:19:22.317562Z"},"trusted":true},"outputs":[],"source":["test_images = test_df[\"img\"].to_numpy()\n","test_resized=[]\n","for im in test_images:\n","    resized = resize_image(im, IMG_HEIGHT, IMG_WIDTH)\n","    test_resized.append(resized)\n","test_resized=np.array(test_resized)\n","y_test_pred_tf = alexnet_reduced.predict(test_resized)\n","y_test_pred=np.where(y_test_pred_tf > 0.4, 1, 0)\n","test_df.loc[:, labels] = y_test_pred\n","\n","show_image_with_label(test_resized,y_test_pred)"]},{"cell_type":"markdown","metadata":{},"source":["## 2.3 Transfer Learning\n","### 2.3.1. Resnet50"]},{"cell_type":"markdown","metadata":{},"source":["In a first transfer learning model, the ResNet50 model is used as an encoder. The classification is done by training two fully connected layers and an output layer."]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2024-05-21T08:45:27.034271Z","iopub.status.busy":"2024-05-21T08:45:27.033656Z","iopub.status.idle":"2024-05-21T08:45:27.039186Z","shell.execute_reply":"2024-05-21T08:45:27.038183Z","shell.execute_reply.started":"2024-05-21T08:45:27.034236Z"},"trusted":true},"outputs":[],"source":["# Define the directory to save the model weights ResNet50\n","weights_folder = './weights/'\n","if not os.path.exists(weights_folder):\n","    os.makedirs(weights_folder)\n","model_weights_path = os.path.join(weights_folder, \"RESNET_2.weights.h5\")"]},{"cell_type":"markdown","metadata":{},"source":["### 2.3.2 Fine-Tuned Models\n","Freezing Layers: Why It’s Useful\n","\n","- **Preserve Learned Features**: Early ResNet50 layers, trained on ImageNet, extract generic features useful for various tasks. Freezing these layers keeps these features intact during training on a new dataset.\n","- **Reduce Training Time**: Training fewer layers decreases computational requirements, speeding up the process since fewer parameters are updated.\n","- **Avoid Overfitting**: It helps prevent overfitting, especially with smaller datasets. Early layers retain general features, while later layers fine-tune to the specific task.\n","\n","Unfreezing Layers: Potential Benefits\n","\n","- **Fine-Tune Specific Features**: By unfreezing some layers, the model can adapt more precisely to the new dataset.\n","- **Balance Preservation and Learning**: In ResNet50, unfreezing layers beyond the 143rd allows the network to fine-tune the last 32 layers. This balances maintaining useful low- and mid-level features while learning high-level features unique to the new data.\n","\n","We’ll unfreeze and train a few layers to try to improve performance.\n","\n","We'll also add dropout and batchnormalization\n"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2024-05-21T08:45:29.130722Z","iopub.status.busy":"2024-05-21T08:45:29.130272Z","iopub.status.idle":"2024-05-21T08:45:31.918393Z","shell.execute_reply":"2024-05-21T08:45:31.917464Z","shell.execute_reply.started":"2024-05-21T08:45:29.130689Z"},"trusted":true},"outputs":[],"source":["def create_resnet_model_2(input_shape, nb_classes):\n","    \"\"\"\n","    Build a ResNet50 model with additional fully connected layers.\n","\n","    Args:\n","    input_shape (tuple): Shape of the input images.\n","    nb_classes (int): Number of classes.\n","\n","    Returns:\n","    model: Keras model.\n","    \"\"\"\n","    resnet_model_2 = Sequential(name=\"ResNetModel_2\")\n","\n","    # Start by importing the pre-defined ResNet50 model\n","    imported_model = tf.keras.applications.ResNet50(include_top=False,\n","                                                    input_shape=input_shape,\n","                                                    pooling='avg',\n","                                                    weights='imagenet')\n","\n","    # Freeze the first 143 layers of the imported model\n","    for layer in imported_model.layers[:143]:\n","        layer.trainable = False\n","\n","    # Unfreeze the remaining layers\n","    for layer in imported_model.layers[143:]:\n","        layer.trainable = True\n","\n","    # Add preprocessing layer\n","    resnet_model_2.add(tf.keras.layers.Lambda(tf.keras.applications.resnet50.preprocess_input, input_shape=input_shape))\n","    resnet_model_2.add(imported_model)\n","\n","    # Add additional layers\n","    \n","    # Complex:\n","\n","    resnet_model_2.add(Flatten())\n","    resnet_model_2.add(Dropout(0.5))\n","    resnet_model_2.add(BatchNormalization())\n","\n","    resnet_model_2.add(Dense(4096, activation='relu'))\n","    resnet_model_2.add(Dropout(0.5))\n","    resnet_model_2.add(BatchNormalization())\n","    resnet_model_2.add(Dense(4096, activation='relu'))\n","    resnet_model_2.add(Dropout(0.5))\n","    resnet_model_2.add(BatchNormalization())\n","\n","\n","    # Keep Simple:\n","\n","    # resnet_model.add(Dense(4096, activation='relu'))\n","    # resnet_model.add(Dense(4096, activation='relu'))\n","    # resnet_model_2.add(BatchNormalization())\n","    # resnet_model_2.add(Dropout(0.5))\n","\n","\n","    resnet_model_2.add(Dense(nb_classes, activation='sigmoid'))\n","\n","    return resnet_model_2\n","\n","def compile_resnet_model_2():\n","    \"\"\"\n","    Compile the ResNet model.\n","\n","    Returns:\n","    model: Compiled Keras model.\n","    \"\"\"\n","    input_shape = (224, 224, 3)\n","    nb_classes = 20\n","\n","    model = create_resnet_model_2(input_shape, nb_classes)\n","    model.compile(optimizer=Adam(learning_rate=0.001), loss='binary_crossentropy', metrics=['Precision', 'Recall', 'Accuracy']) # Try changing the learning rate \n","    model.summary()\n","    return model\n","\n","# Compile the ResNet model\n","resnet_model_2 = compile_resnet_model_2()"]},{"cell_type":"markdown","metadata":{},"source":["We use early stopping during training to prevent overfitting."]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2024-05-20T19:52:20.689173Z","iopub.status.busy":"2024-05-20T19:52:20.688512Z","iopub.status.idle":"2024-05-20T19:59:06.295038Z","shell.execute_reply":"2024-05-20T19:59:06.293963Z","shell.execute_reply.started":"2024-05-20T19:52:20.689132Z"},"trusted":true},"outputs":[],"source":["# Define number of epochs\n","epochs = 100\n","\n","# Get callbacks\n","early_stopping = EarlyStopping(monitor='val_Accuracy', min_delta=0.01, patience=epochs//5, mode='min')\n","\n","# Assuming X_train and y_train are defined and preprocessed\n","history_resnet_2 = resnet_model_2.fit(\n","    X_train,\n","    y_train,\n","    validation_split=0.2,\n","    epochs=epochs,\n","    batch_size=16,\n","    callbacks=[early_stopping]  # Include the callbacks in the training process\n",")"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2024-05-20T20:08:56.449850Z","iopub.status.busy":"2024-05-20T20:08:56.448866Z","iopub.status.idle":"2024-05-20T20:08:56.484630Z","shell.execute_reply":"2024-05-20T20:08:56.483433Z","shell.execute_reply.started":"2024-05-20T20:08:56.449811Z"},"trusted":true},"outputs":[],"source":["plot_history(history_resnet_2)"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2024-05-20T20:00:20.459620Z","iopub.status.busy":"2024-05-20T20:00:20.458818Z","iopub.status.idle":"2024-05-20T20:00:21.732625Z","shell.execute_reply":"2024-05-20T20:00:21.731697Z","shell.execute_reply.started":"2024-05-20T20:00:20.459587Z"},"trusted":true},"outputs":[],"source":["# Save the model weights\n","resnet_model_2.save_weights(model_weights_path)"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2024-05-21T08:45:38.970447Z","iopub.status.busy":"2024-05-21T08:45:38.969706Z","iopub.status.idle":"2024-05-21T08:45:41.652925Z","shell.execute_reply":"2024-05-21T08:45:41.651906Z","shell.execute_reply.started":"2024-05-21T08:45:38.970414Z"},"trusted":true},"outputs":[],"source":["# Load the model and weights for further use\n","# Compile the VGG model\n","input_shape = (224, 224, 3)\n","nb_classes = 20\n","resnet_model_2 = compile_resnet_model_2()\n","resnet_model_2.load_weights(model_weights_path)"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2024-05-21T08:48:19.349892Z","iopub.status.busy":"2024-05-21T08:48:19.349031Z","iopub.status.idle":"2024-05-21T08:48:19.364050Z","shell.execute_reply":"2024-05-21T08:48:19.363172Z","shell.execute_reply.started":"2024-05-21T08:48:19.349860Z"},"trusted":true},"outputs":[],"source":["from sklearn.preprocessing import LabelEncoder\n","\n","# Assuming you have a list of class names\n","class_names = ['aeroplane', 'bicycle', 'bird', 'boat', 'bottle', \n","               'bus', 'car', 'cat', 'chair', 'cow', 'diningtable', \n","               'dog', 'horse', 'motorbike', 'person', 'pottedplant', \n","               'sheep', 'sofa', 'train', 'tvmonitor']\n","\n","# Create and fit the LabelEncoder\n","encoder = LabelEncoder()\n","encoder.fit(class_names)"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2024-05-21T08:53:26.488845Z","iopub.status.busy":"2024-05-21T08:53:26.488203Z","iopub.status.idle":"2024-05-21T08:53:38.707613Z","shell.execute_reply":"2024-05-21T08:53:38.706627Z","shell.execute_reply.started":"2024-05-21T08:53:26.488813Z"},"trusted":true},"outputs":[],"source":["from sklearn.metrics import classification_report, average_precision_score, accuracy_score\n","# Assuming X_train, y_train, X_val, y_val, X_test, y_test are defined and preprocessed\n","\n","# Function to get classification metrics\n","def print_classification_metrics(model, X_test, y_test, encoder):\n","    preds = np.round(model.predict(X_test), 0)\n","    classification_metrics = classification_report(y_test, preds, target_names=encoder.classes_)\n","    print(classification_metrics)\n","    print(\"mAP: {:.2f}%\".format(average_precision_score(y_test, preds) * 100))\n","\n","# Display the classification metrics for the validation set\n","print_classification_metrics(resnet_model_2, X_train, y_train, encoder)"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2024-05-21T08:53:52.891654Z","iopub.status.busy":"2024-05-21T08:53:52.891294Z","iopub.status.idle":"2024-05-21T08:53:59.285651Z","shell.execute_reply":"2024-05-21T08:53:59.284666Z","shell.execute_reply.started":"2024-05-21T08:53:52.891627Z"},"trusted":true},"outputs":[],"source":["y_pred_prob  = resnet_model_2.predict(X_test)\n","\n","# Experiment with the following threshold to get\n","# the best f1 score. Should be around 0.3-0.5. This is based on previous experiments using test set (not used anymore)\n","threshold = 0.3\n","\n","y_pred = np.array(y_pred_prob>threshold, int)\n","test_df.loc[:, labels] = y_pred\n","test_df.head(1)"]},{"cell_type":"markdown","metadata":{},"source":["VGG16 was also utilized in an attempt to potentially yield better results.\n","\n","**Key differences:**\n","\n","*  **Depth**: ResNet50 is deeper with 50 layers, whereas VGG16 consists of 16 layers.\n","*  **Architecture**: ResNet50 employs residual connections to facilitate the training of deeper networks, while VGG16 follows a straightforward, sequential architecture without shortcuts.\n","\n","Despite these efforts, no improvement in results was observed. We also attempted unfreezing some layers, but this did not lead to any enhancements either.\n","\n"]},{"cell_type":"markdown","metadata":{},"source":["# 3. Semantic segmentation\n","The goal here is to implement a segmentation CNN that labels every pixel in the image as belonging to one of the 20 classes (or background). We first define some functions to show predictions and the final submission classification/segmentation."]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2024-05-21T08:54:13.571316Z","iopub.status.busy":"2024-05-21T08:54:13.570681Z","iopub.status.idle":"2024-05-21T08:54:13.578714Z","shell.execute_reply":"2024-05-21T08:54:13.577552Z","shell.execute_reply.started":"2024-05-21T08:54:13.571273Z"},"trusted":true},"outputs":[],"source":["def show_predictions(X, Y_pred, indices, ground_truth=None):\n","    m = len(indices)\n","    f = 2 if ground_truth is None else 3\n","    for i in range(m):\n","        plt.subplot(m, f, i*f+1)\n","        plt.imshow(X[indices[i]])\n","        plt.subplot(m, f, i*f+2)\n","        plt.imshow(Y_pred[indices[i]],vmin=0, vmax=20)\n","        if ground_truth is not None:\n","            plt.subplot(m, f, i*f+3)\n","            plt.imshow(ground_truth[indices[i]],vmin=0, vmax=20)\n","    plt.axis('off')\n","    plt.show()"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2024-05-21T08:54:15.805709Z","iopub.status.busy":"2024-05-21T08:54:15.804871Z","iopub.status.idle":"2024-05-21T08:54:15.816050Z","shell.execute_reply":"2024-05-21T08:54:15.814923Z","shell.execute_reply.started":"2024-05-21T08:54:15.805668Z"},"trusted":true},"outputs":[],"source":["def show_submission(df,N):\n","    CLASSES = {'aeroplane': 0, 'bicycle': 1, 'bird': 2, 'boat': 3, 'bottle': 4, 'bus': 5,\n","            'car': 6, 'cat': 7, 'chair': 8, 'cow': 9, 'diningtable': 10, 'dog': 11,\n","            'horse': 12, 'motorbike':13, 'person': 14, 'pottedplant': 15, 'sheep': 16,\n","            'sofa': 17, 'train': 18, 'tvmonitor': 19}\n","\n","    for i in range(N):\n","        plt.subplot(1,2,1)\n","        plt.imshow(df[\"img\"][i])\n","        plt.subplot(1, 2, 2)\n","        mask = df[\"seg\"][i]\n","        print(f\"number of different segmented classes in img {i} = {np.unique(mask)}\")\n","        plt.imshow(df[\"seg\"][i],vmin=0, vmax=20)\n","        class_names = [class_name for class_name, label in CLASSES.items() if df.loc[i][label] == 1]\n","        plt.title(\",\".join(class_names))\n","        plt.show()\n","        \n","    "]},{"cell_type":"markdown","metadata":{},"source":["We can already see what the classification has classified the testdata with."]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2024-05-21T08:54:42.287144Z","iopub.status.busy":"2024-05-21T08:54:42.286735Z","iopub.status.idle":"2024-05-21T08:54:46.071528Z","shell.execute_reply":"2024-05-21T08:54:46.070611Z","shell.execute_reply.started":"2024-05-21T08:54:42.287096Z"},"trusted":true},"outputs":[],"source":["show_submission(test_df,10)"]},{"cell_type":"markdown","metadata":{},"source":["We shall resize and normalize the images while only resizing the masks. We must make sure the labels in the segmentation masks are still correct labels and that the resizing happens in a logical way by taking a value from neighbouring pixels instead of an average."]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2024-05-21T08:54:21.132743Z","iopub.status.busy":"2024-05-21T08:54:21.131907Z","iopub.status.idle":"2024-05-21T08:54:21.143722Z","shell.execute_reply":"2024-05-21T08:54:21.142549Z","shell.execute_reply.started":"2024-05-21T08:54:21.132704Z"},"trusted":true},"outputs":[],"source":["def resize_images(images,img_size):\n","    list_images = list()\n","    for img in images:\n","        normal_image = tf.cast(img, tf.float32) / 255.0\n","        img_resized = resize(normal_image,img_size, anti_aliasing=True)\n","        list_images.append(img_resized)\n","    return np.array(list_images) \n","\n","def resize_masks(masks,mask_size):\n","    list_masks = list()\n","    for mask in masks:\n","        mask_resized = tf.image.resize(mask[None,...,None],mask_size, method  =tf.image.ResizeMethod.NEAREST_NEIGHBOR )\n","        mask_resized = np.clip(mask_resized,0,len(labels))\n","        list_masks.append(mask_resized[0,:,:,0])\n","    return np.array(list_masks) \n"]},{"cell_type":"markdown","metadata":{},"source":["We now resize the training set and split off 20% for validation purposes."]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2024-05-20T21:56:11.386268Z","iopub.status.busy":"2024-05-20T21:56:11.385652Z","iopub.status.idle":"2024-05-20T21:56:25.126229Z","shell.execute_reply":"2024-05-20T21:56:25.125188Z","shell.execute_reply.started":"2024-05-20T21:56:11.386238Z"},"trusted":true},"outputs":[],"source":["NUM_LABELS = len(labels) + 1\n","\n","X_train_seg = resize_images(train_df[\"img\"],(IMG_HEIGHT,IMG_WIDTH,3))\n","Y_train_seg = resize_masks(train_df[\"seg\"],(IMG_HEIGHT,IMG_WIDTH))\n","\n","X_train_seg, X_val_seg, Y_train_seg, Y_val_seg = train_test_split(X_train_seg,Y_train_seg,test_size=0.2,random_state=42)"]},{"cell_type":"markdown","metadata":{},"source":["We also augment the data set, we found that only flipping images yields better results for the segmentation than also changing the brightness like in the classification case."]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2024-05-20T21:56:28.640327Z","iopub.status.busy":"2024-05-20T21:56:28.639966Z","iopub.status.idle":"2024-05-20T21:56:34.121719Z","shell.execute_reply":"2024-05-20T21:56:34.120755Z","shell.execute_reply.started":"2024-05-20T21:56:28.640300Z"},"trusted":true},"outputs":[],"source":["def augment(X, Y):\n","  flip = np.random.random(X.shape[0])\n","  X_new = np.empty((X.shape[0] + np.sum(flip > 0.5), X.shape[1],X.shape[2],X.shape[3]), dtype=X.dtype)\n","  Y_new = np.empty((Y.shape[0] + np.sum(flip > 0.5), Y.shape[1],Y.shape[2]),dtype=Y.dtype)\n","  ind = 0\n","  for i in range(X.shape[0]):\n","    X_new[ind] = X[i]\n","    Y_new[ind] = Y[i]\n","    ind += 1\n","    if flip[i]>0.5:\n","      X_new[ind] = np.fliplr(X[i])\n","      Y_new[ind] = np.fliplr(Y[i])\n","      ind += 1\n","  \n","  return X_new, Y_new\n","  \n","X_aug_seg, Y_aug_seg = augment(X_train_seg,Y_train_seg)\n","np.unique(Y_aug_seg)\n","\n","TRAIN_LENGTH = X_train_seg.shape[0]\n","BATCH_SIZE = 32\n","BUFFER_SIZE = 1500\n","STEPS_PER_EPOCH = TRAIN_LENGTH // BATCH_SIZE\n","\n","training_set = tf.data.Dataset.from_tensor_slices((X_aug_seg, Y_aug_seg))\n","val_set = tf.data.Dataset.from_tensor_slices((X_val_seg,Y_val_seg))\n","train_batches = (\n","    training_set\n","    .cache()\n","    .shuffle(BUFFER_SIZE)\n","    .batch(BATCH_SIZE)\n","    .repeat()\n","    .prefetch(buffer_size=tf.data.AUTOTUNE))\n","\n","val_batches = val_set.batch(BATCH_SIZE)\n","for images, masks in train_batches.take(4):\n","    sample_image, sample_mask = images[0], masks[0]\n","    display((sample_image,sample_mask))"]},{"cell_type":"markdown","metadata":{},"source":["We shall set up a display callback to see the changes in segmentation during the training."]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2024-05-20T21:56:36.998236Z","iopub.status.busy":"2024-05-20T21:56:36.997767Z","iopub.status.idle":"2024-05-20T21:56:37.007422Z","shell.execute_reply":"2024-05-20T21:56:37.006489Z","shell.execute_reply.started":"2024-05-20T21:56:36.998207Z"},"trusted":true},"outputs":[],"source":["EPOCHS = 30\n","VAL_SUBSPLITS = 5\n","VALIDATION_STEPS = X_val_seg.shape[0]//BATCH_SIZE//VAL_SUBSPLITS\n","\n","\n","class DisplayCallback(tf.keras.callbacks.Callback):\n","  def on_epoch_end(self, epoch, logs=None):\n","        clear_output(wait=True)\n","        indices = [6]\n","        for i in range(len(indices)):\n","            pred = model.predict(X_val_seg[indices[i]:indices[i]+1])\n","            pred = tf.math.argmax(pred, axis=-1)\n","            plt.subplot(1, 3, 1)\n","            plt.imshow(X_val_seg[indices[i]])\n","            plt.subplot(1, 3, 2)\n","            plt.imshow(Y_val_seg[indices[i]],vmin=0, vmax=20)\n","            plt.subplot(1, 3, 3)\n","            plt.imshow(pred[0],vmin=0, vmax=20)\n","            plt.axis('off')\n","            plt.show()\n","        print ('\\nSample Prediction after epoch {}\\n'.format(epoch+1))"]},{"cell_type":"markdown","metadata":{},"source":["We shall use the mean dice metric to see how well we can expect our segmentation to perform on the test set, since the competition also uses a dice metric."]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2024-05-20T21:56:42.567199Z","iopub.status.busy":"2024-05-20T21:56:42.566323Z","iopub.status.idle":"2024-05-20T21:56:42.573962Z","shell.execute_reply":"2024-05-20T21:56:42.572841Z","shell.execute_reply.started":"2024-05-20T21:56:42.567167Z"},"trusted":true},"outputs":[],"source":["def mean_dice_coefficient(y_true, y_pred, smooth=1.0):\n","    \n","    mean_dice = 0\n","    for i in range(len(labels)):\n","        mask_true = y_true == i\n","        mask_pred = tf.math.argmax(y_pred, axis=-1) == i\n","        intersection = tf.cast(tf.reduce_sum(tf.cast(tf.logical_and(mask_true, mask_pred), tf.int32)), tf.float32)\n","        mean_dice += (2.*intersection+smooth)/(1.+tf.cast(tf.reduce_sum(tf.cast(mask_true, tf.int32)) + tf.reduce_sum(tf.cast(mask_pred, tf.int32)), tf.float32))\n","    mean_dice /=len(labels)\n","    return mean_dice"]},{"cell_type":"markdown","metadata":{},"source":["We shall use a U-net for our segmentation.\n","A U-net consists of an encoder followed by a bottle neck and a decoder. The encoder and decoder are connected using skip-connections.\n","### contraction\n","The image is first \"contracted\" using an encoder. The encoder detects features on multiple levels by iteratively convoluting the image with a small convolution matrix at each layer. This way the image is halved each time while the feature channel doubles each time. It consists of multiple layers each containing two 3x3 convolutional layers followed by a ReLU activation and a downsampling.\n","### bottleneck\n","Here the image is in its most contracted state and features are learned on the most abstract level.\n","### expansion\n","After the bottleneck the original image is recreated by \"decoding\" the features learned at the coarsest level. It is a symmetric reversal of the contraction path and must recreate the full features at the larger image scales.\n","### skip connections\n","Skip connections connect the output of the encoder layers to the input of the corresponding decoder layers. They help reconstruct the features.\n"]},{"cell_type":"markdown","metadata":{},"source":["We shall use transfer learning for our best model. For this we use a pretrained model for the encoding step and build a decoder on top of this. We will use the pretrained weights on the ImageNet dataset for the MobilNetV2 model. We also, initialy, freeze the weights of the pretrained model when we train our U-net."]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2024-05-20T21:56:44.739605Z","iopub.status.busy":"2024-05-20T21:56:44.738718Z","iopub.status.idle":"2024-05-20T21:56:46.208172Z","shell.execute_reply":"2024-05-20T21:56:46.207167Z","shell.execute_reply.started":"2024-05-20T21:56:44.739574Z"},"trusted":true},"outputs":[],"source":["base_model = tf.keras.applications.MobileNetV2(input_shape=[IMG_HEIGHT, IMG_WIDTH, 3],weights='imagenet', include_top=False)\n","layer_names = [\n","    'block_1_expand_relu', \n","    'block_3_expand_relu',  \n","    'block_6_expand_relu', \n","    'block_13_expand_relu', \n","    'block_16_project',      \n","]\n","base_model_outputs = [base_model.get_layer(name).output for name in layer_names]\n","\n","# Create the feature extraction model\n","down_stack = tf.keras.Model(inputs=base_model.input, outputs=base_model_outputs)\n","\n","\n","down_stack.trainable = False\n","\n","def upsample(filters, size, apply_dropout=False):\n","  initializer = tf.random_normal_initializer(0., 0.02)\n","\n","  result = tf.keras.Sequential()\n","  result.add(\n","    tf.keras.layers.Conv2DTranspose(filters, size, strides=2,\n","                                    padding='same',\n","                                    kernel_initializer=initializer,\n","                                    use_bias=False))\n","\n","  result.add(tf.keras.layers.BatchNormalization())\n","\n","  if apply_dropout:\n","      result.add(tf.keras.layers.Dropout(0.5))\n","\n","  result.add(tf.keras.layers.ReLU())\n","\n","  return result\n","\n","up_stack = [\n","    upsample(512, 3),  # 4x4 -> 8x8\n","    upsample(256, 3),  # 8x8 -> 16x16\n","    upsample(128, 3),  # 16x16 -> 32x32\n","    upsample(64, 3),   # 32x32 -> 64x64\n","]\n","\n","def unet_model(output_channels:int):\n","    inputs = tf.keras.layers.Input(shape=[IMG_HEIGHT, IMG_WIDTH, 3])\n","\n","    # Downsampling through the model\n","    skips = down_stack(inputs)\n","    x = skips[-1]\n","    skips = reversed(skips[:-1])\n","\n","    # Upsampling and establishing the skip connections\n","    for up, skip in zip(up_stack, skips):\n","        x = up(x)\n","        concat = tf.keras.layers.Concatenate()\n","        x = concat([x, skip])\n","\n","    # This is the last layer of the model\n","    last = tf.keras.layers.Conv2DTranspose(\n","        filters=output_channels, kernel_size=3, strides=2,\n","        padding='same')  #64x64 -> 128x128\n","\n","    x = last(x)\n","\n","    return tf.keras.Model(inputs=inputs, outputs=x)\n","OUTPUT_CLASSES = len(labels)+1\n","\n","model = unet_model(output_channels=OUTPUT_CLASSES)\n","model.compile(optimizer='adam',\n","              loss=tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True),\n","              metrics=['accuracy', mean_dice_coefficient])\n","\n","model.summary()"]},{"cell_type":"markdown","metadata":{},"source":["There is a large imbalance in the trainingset. Most pixels in the training set are background (about 77%). If left unattended, this can lead to the model only learning to segment background. Since images exist mostly out of background, we will inversely weight pixels representing background in the training set, this way while each background pixel occurs more frequently they are also made less important."]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2024-05-20T21:56:48.613550Z","iopub.status.busy":"2024-05-20T21:56:48.612683Z","iopub.status.idle":"2024-05-20T21:56:49.140599Z","shell.execute_reply":"2024-05-20T21:56:49.139672Z","shell.execute_reply.started":"2024-05-20T21:56:48.613518Z"},"trusted":true},"outputs":[],"source":["distribution = np.mean([[np.sum(Y_ == i) / Y_.size for i in range(len(labels) + 1)] for Y_ in Y_train_seg], axis=0)\n","def add_sample_weights(image, label):\n","  # The weights for each class, with the constraint that:\n","  #     sum(class_weights) == 1.0\n","  class_weights = tf.constant(1/distribution)\n","#   temp = 20\n","#   class_weights = np.array([1,temp, temp,temp, temp,temp, temp,temp, temp,temp, temp,temp, temp,temp, temp,temp, temp,temp, temp,temp, temp])\n","  class_weights = class_weights/tf.reduce_sum(class_weights)\n","\n","  # Create an image of `sample_weights` by using the label at each pixel as an\n","  # index into the `class weights` .\n","  sample_weights = tf.gather(class_weights, indices=tf.cast(label, tf.int32))\n","\n","  return image, label, sample_weights"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["model_weights_path = os.path.join(weights_folder, \"mobilenetv2.weights.h5\")"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2024-05-20T21:56:52.029057Z","iopub.status.busy":"2024-05-20T21:56:52.028693Z","iopub.status.idle":"2024-05-20T22:00:13.269721Z","shell.execute_reply":"2024-05-20T22:00:13.268842Z","shell.execute_reply.started":"2024-05-20T21:56:52.029029Z"},"trusted":true},"outputs":[],"source":["model_history = model.fit(train_batches.map(add_sample_weights), epochs=EPOCHS,\n","                          steps_per_epoch=STEPS_PER_EPOCH,\n","                          validation_steps=VALIDATION_STEPS,\n","                          validation_data=val_batches,\n","                          callbacks=[DisplayCallback()])"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2024-05-20T22:00:19.268010Z","iopub.status.busy":"2024-05-20T22:00:19.267412Z","iopub.status.idle":"2024-05-20T22:00:19.923961Z","shell.execute_reply":"2024-05-20T22:00:19.922972Z","shell.execute_reply.started":"2024-05-20T22:00:19.267979Z"},"trusted":true},"outputs":[],"source":["loss = model_history.history['loss']\n","val_loss = model_history.history['val_loss']\n","\n","plt.figure()\n","plt.plot(model_history.epoch, loss, 'r', label='Training loss')\n","plt.plot(model_history.epoch, val_loss, 'bo', label='Validation loss')\n","plt.title('Training and Validation Loss')\n","plt.xlabel('Epoch')\n","plt.ylabel('Loss Value')\n","plt.ylim([0, 1])\n","plt.legend()\n","plt.show()"]},{"cell_type":"markdown","metadata":{},"source":["While the sample weighting helps in preventing the background from taking over during the training, training some more on a non-weighted training set. This will help get rid of blobs outside of the actual images (by not reweighting the background we give it a higher importance, thus leading to more background)"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2024-05-20T21:34:41.556414Z","iopub.status.busy":"2024-05-20T21:34:41.555508Z","iopub.status.idle":"2024-05-20T21:34:46.895540Z","shell.execute_reply":"2024-05-20T21:34:46.894345Z","shell.execute_reply.started":"2024-05-20T21:34:41.556382Z"},"trusted":true},"outputs":[],"source":["model_history = model.fit(train_batches, epochs=20,\n","                          steps_per_epoch=STEPS_PER_EPOCH,\n","                          validation_steps=VALIDATION_STEPS,\n","                          validation_data=val_batches,\n","                          callbacks=[DisplayCallback()])"]},{"cell_type":"markdown","metadata":{},"source":["Finally we finetune the model by making the pretrained layers trainable and training some more with a very low training rate. This adapts the pretrained encoder model for better meshing with our decoder. "]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2024-05-20T21:24:49.975545Z","iopub.status.busy":"2024-05-20T21:24:49.975171Z","iopub.status.idle":"2024-05-20T21:28:09.231188Z","shell.execute_reply":"2024-05-20T21:28:09.230251Z","shell.execute_reply.started":"2024-05-20T21:24:49.975515Z"},"trusted":true},"outputs":[],"source":["down_stack.trainable = True\n","model.compile(optimizer=tf.keras.optimizers.Adam(1e-5),  # Very low learning rate\n","              loss=tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True),\n","              metrics=['accuracy'])\n","model_history = model.fit(train_batches, epochs=20,\n","                          steps_per_epoch=STEPS_PER_EPOCH,\n","                          validation_steps=VALIDATION_STEPS,\n","                          validation_data=val_batches,\n","                          callbacks=[DisplayCallback()])"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["model.save_weights(model_weights_path)"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["model.load_weights(model_weights_path)"]},{"cell_type":"markdown","metadata":{},"source":["We now segment the test set and view the results."]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2024-05-20T22:00:29.362922Z","iopub.status.busy":"2024-05-20T22:00:29.362281Z","iopub.status.idle":"2024-05-20T22:01:02.596934Z","shell.execute_reply":"2024-05-20T22:01:02.595944Z","shell.execute_reply.started":"2024-05-20T22:00:29.362890Z"},"trusted":true},"outputs":[],"source":["X_test_seg = resize_images(test_df[\"img\"],(IMG_HEIGHT,IMG_WIDTH, 3))\n","res = model.predict(X_test_seg)\n","masks = np.argmax(res,axis=-1).astype(np.uint8)\n","test_df[\"seg\"] = [tf.reshape(tf.image.resize(masks[i][None,...,None],(img.shape[0],img.shape[1]), method=tf.image.ResizeMethod.NEAREST_NEIGHBOR),(img.shape[0],img.shape[1])) for i,img in enumerate(test_df[\"img\"])]"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2024-05-20T22:06:56.686874Z","iopub.status.busy":"2024-05-20T22:06:56.686535Z","iopub.status.idle":"2024-05-20T22:07:00.701075Z","shell.execute_reply":"2024-05-20T22:07:00.700131Z","shell.execute_reply.started":"2024-05-20T22:06:56.686850Z"},"trusted":true},"outputs":[],"source":["show_submission(test_df,10)"]},{"cell_type":"markdown","metadata":{},"source":["There are some nicely segmented objects, although a couple things can be noticed:\n","    - Some images have no classification\n","    - Images can have a lot of segmented parts of only a couple pixels\n","    - Sometimes the classification detects multiple items that are not really present in the segmentation\n","\n","We will attempt to fix these issues by postprocessing the segmentation and classification:\n","    - If there is no classification we will pick the classes that are well represented in the segmentation (>20% of non-background)\n","    - If a segmentation class is not in the classification and the number of pixels is low (<5%) we will set is to background, unless there is only one class in the classification in this case all small non-background segmentations are set to the classification class\n","    - If a classification class is not well represented in the segmentation (<5%) then remove the classification"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["def postprocess(df):\n","    CLASSES = {'aeroplane': 0, 'bicycle': 1, 'bird': 2, 'boat': 3, 'bottle': 4, 'bus': 5,\n","            'car': 6, 'cat': 7, 'chair': 8, 'cow': 9, 'diningtable': 10, 'dog': 11,\n","            'horse': 12, 'motorbike':13, 'person': 14, 'pottedplant': 15, 'sheep': 16,\n","            'sofa': 17, 'train': 18, 'tvmonitor': 19}\n","    res = []\n","    for i in range(len(df)):\n","        class_names = [class_name for class_name, label in CLASSES.items() if df.loc[i,class_name] == 1]\n","        mask = df.loc[i, \"seg\"]\n","        tol = 0.05\n","        new_mask = tf.identity(mask)\n","        # remove small segmentations\n","        for j in range(len(CLASSES)):\n","            if (tf.reduce_sum(tf.cast(mask == (j + 1), tf.float32)) < tol * tf.reduce_sum(tf.cast(mask !=0, tf.float32))):\n","    #                     print(f\"image only has {tf.reduce_sum(tf.cast(mask == (j + 1), tf.float32))} pixels of {list(CLASSES.keys())[j]} ({float(len(np.where(mask == (j+1)[0]))/tf.reduce_sum(tf.cast(mask !=0, tf.float32))}%)\")\n","                new_mask = tf.where(mask == (j + 1), 0, new_mask)\n","        \n","        if (len(class_names) > 1):\n","            # if you detected multiple classes, but the segmentation did not, remove the classification \n","            tol = 0.05\n","            j = 1        \n","            for class_name, label in CLASSES.items():\n","                if (df.loc[i,class_name] == 1) and( tf.reduce_sum(tf.cast(mask == (j), tf.float32)) < tol*tf.reduce_sum(tf.cast(mask !=0, tf.float32))):\n","#                     print(f\"image of {tf.size(mask).numpy()} pixels, only has {len(np.where(mask == j)[0])} pixels of {class_name} detected ({float(len(np.where(mask == j)[0]))/tf.reduce_sum(tf.cast(mask !=0, tf.float32))}%)\")\n","                    df.loc[i,class_name] = 0\n","                j +=1 \n","        # if you only detected one class, and there are small sections of others segmented, change them into the colour\n","        elif (len(class_names) == 1):\n","            tol = 0.15\n","            for j in range(len(CLASSES)):\n","                if (CLASSES[class_names[0]] != j) and (tf.reduce_sum(tf.cast(mask == (j + 1), tf.float32)) < tol * tf.reduce_sum(tf.cast(mask !=0, tf.float32))):\n","                    new_mask = tf.where(mask == (j + 1), CLASSES[class_names[0]] + 1, new_mask)\n","        else:\n","            # if you detected multiple classes, but the segmentation did not, remove the classification \n","            tol = 0.05\n","            # remove small segmentations\n","            for j in range(len(CLASSES)):\n","                # no classification but obvious segmentation\n","                if (len(class_names) == 0) and (tf.reduce_sum(tf.cast(mask == (j + 1), tf.float32)) > 0.2 * tf.reduce_sum(tf.cast(mask !=0, tf.float32))):\n","#                     print(f\"image {i} has nothing classified, but segmentation of {tf.reduce_sum(tf.cast(mask == (j + 1), tf.float32))/tf.reduce_sum(tf.cast(mask !=0, tf.float32))} for {list(CLASSES.keys())[j]}\")\n","                    df.loc[i,list(CLASSES.keys())[j]] = 1\n","        res.append(new_mask)\n","    df[\"seg\"] = res\n","    return df"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2024-05-20T22:08:14.614189Z","iopub.status.busy":"2024-05-20T22:08:14.613815Z","iopub.status.idle":"2024-05-20T22:08:16.438128Z","shell.execute_reply":"2024-05-20T22:08:16.437339Z","shell.execute_reply.started":"2024-05-20T22:08:14.614162Z"},"trusted":true},"outputs":[],"source":["test_df = postprocess(test_df)"]},{"cell_type":"markdown","metadata":{},"source":["We see that these issues are mostly resolved."]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2024-05-20T22:08:18.664689Z","iopub.status.busy":"2024-05-20T22:08:18.664055Z","iopub.status.idle":"2024-05-20T22:08:23.042186Z","shell.execute_reply":"2024-05-20T22:08:23.041260Z","shell.execute_reply.started":"2024-05-20T22:08:18.664658Z"},"trusted":true},"outputs":[],"source":["show_submission(test_df,10)"]},{"cell_type":"markdown","metadata":{},"source":["# Submission"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2024-05-20T22:08:31.365312Z","iopub.status.busy":"2024-05-20T22:08:31.364355Z","iopub.status.idle":"2024-05-20T22:08:41.221337Z","shell.execute_reply":"2024-05-20T22:08:41.220360Z","shell.execute_reply.started":"2024-05-20T22:08:31.365262Z"},"trusted":true},"outputs":[],"source":["generate_submission(test_df)"]},{"cell_type":"markdown","metadata":{},"source":["# 4. Adversarial attack\n","For this part, your goal is to fool your classification and/or segmentation CNN, using an *adversarial attack*. More specifically, the goal is build a CNN to perturb test images in a way that (i) they look unperturbed to humans; but (ii) the CNN classifies/segments these images in line with the perturbations."]},{"cell_type":"markdown","metadata":{},"source":["This part demonstrates how to generate targeted adversarial examples using our classification model resnet 2. The goal is to perturb input images such that the model misclassifies them as a specified target class."]},{"cell_type":"markdown","metadata":{},"source":["## 4.1.1. Define Functions\n","\n","### targeted_adversarial_pattern\n","\n","This function generates the adversarial perturbation for a given input image and target label. It uses the gradient of the loss with respect to the input image to create the perturbation."]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["def targeted_adversarial_pattern(input_image, target_label):\n","    input_image = tf.convert_to_tensor(input_image, dtype=tf.float32)\n","    target_label = tf.convert_to_tensor(target_label, dtype=tf.float32)\n","    target_label = tf.reshape(target_label, (1, -1))  # Ensure the target label shape matches the prediction\n","    with tf.GradientTape() as tape:\n","        tape.watch(input_image)\n","        prediction = resnet_model_2(input_image)\n","        loss = tf.keras.losses.binary_crossentropy(target_label, prediction)\n","    gradient = tape.gradient(loss, input_image)\n","    signed_grad = tf.sign(gradient)  # Get the sign of the gradient to create the perturbation\n","    return signed_grad.numpy()"]},{"cell_type":"markdown","metadata":{},"source":["### scale_perturbations\n","\n","This function scales the perturbations for better visualization."]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["def scale_perturbations(perturbations):\n","    perturbations = perturbations - perturbations.min()\n","    if perturbations.max() != 0:\n","        perturbations = perturbations / perturbations.max()\n","    return perturbations"]},{"cell_type":"markdown","metadata":{},"source":["## 4.1.2. Set Parameters\n","\n","Set the parameters for the adversarial attack."]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["c = 0\n","target_class_index = 18  # The index of the target class to be set to 1\n","num_samples = 2  # Number of samples to attack\n","epsilon = 0.45  # Perturbation step size\n","num_steps = 10  # Number of iterative steps"]},{"cell_type":"markdown","metadata":{},"source":["## 4.1.3. Iterate Through Samples\n","\n","For each sample, we begin by getting the original prediction from the model. We then create a one-hot encoded target label vector representing the desired misclassification. We start with the original image and we iteratively apply perturbations: in each step, we compute the gradient of the loss (between the model’s prediction and the target label) with respect to the image, we generate perturbations, and adjust the image by subtracting the perturbations multiplied with a factor epsilon. This is repeated for a specified number of steps. After applying all perturbations, we obtain the final adversarial prediction. We visualize the original image, the perturbations, and the adversarial image by plotting them side-by-side."]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["# Iterate through the samples\n","for i in range(num_samples):\n","    num = i\n","    labels = list(train_df.columns[:-2])\n","    image = X_train[num]\n","    image_label = y_train[num]\n","\n","    # Get the original prediction\n","    scores_ori = resnet_model_2.predict(image.reshape((1, 224, 224, 3)))\n","    pred_ori = [labels[idx] for idx, val in enumerate(scores_ori[0]) if val > 0.5]\n","    \n","    # Create the desired label vector based on the original label but with high confidence in the target class\n","    des_label = image_label.copy()\n","    des_label[target_class_index] = 1  # Set high confidence in the target class\n","\n","    # Ensure the desired label is reshaped correctly\n","    des_label = des_label.reshape((1, -1))  # Reshape to match the output shape\n","\n","    # Start with the original image\n","    adversarial = image.copy()\n","\n","    # Iteratively apply perturbations\n","    for step in range(num_steps):\n","        perturbations = targeted_adversarial_pattern(adversarial.reshape((1, 224, 224, 3)), des_label)\n","        adversarial = adversarial - epsilon * perturbations  # Subtraction to move towards the target class\n","        adversarial = np.clip(adversarial, 0, 255)  # Ensure pixel values are in the valid range\n","\n","    # Get the final adversarial prediction\n","    scores_adv = resnet_model_2.predict(adversarial.reshape((1, 224, 224, 3)))\n","    pred_adv = [labels[idx] for idx, val in enumerate(scores_adv[0]) if val > 0.5]\n","\n","    # Remove the batch dimension for visualization\n","    adversarial_img = adversarial.reshape((224, 224, 3))\n","    perturbation_image = (adversarial - image).reshape((224, 224, 3))\n","    \n","    # Scale perturbations for better visualization\n","    scaled_perturbations = scale_perturbations(perturbation_image)\n","\n","    # Display original image, adversarial image, and perturbations\n","    fig, ax = plt.subplots(nrows=1, ncols=3, figsize=(15, 5))\n","\n","    # Original image\n","    ax[0].imshow(image / 255.0)\n","    ax[0].set_title(f'Original Image\\nPrediction: {pred_ori}')\n","    ax[0].axis('off')\n","\n","    # Perturbations\n","    ax[1].imshow(scaled_perturbations)\n","    ax[1].set_title('Perturbations')\n","    ax[1].axis('off')\n","    \n","    # Adversarial image\n","    ax[2].imshow(adversarial_img / 255.0)\n","    ax[2].set_title(f'Adversarial Image\\nPrediction: {pred_adv}')\n","    ax[2].axis('off')\n","\n","    plt.show()\n","\n","    # Check if the adversarial attack was successful\n","    if pred_ori != pred_adv and scores_adv[0][target_class_index] > 0.5:\n","        c += 1\n","        print(f'Successful attack! Original Prediction: {pred_ori}, Adversarial Prediction: {pred_adv}')\n","    else:\n","        print(f'Unsuccessful attack. Original Prediction: {pred_ori}, Adversarial Prediction: {pred_adv}')\n","\n","print(f'Number of successful adversarial attacks: {c}')\n"]},{"cell_type":"markdown","metadata":{},"source":["## 4.1.4 Discussion results\n","\n","We demonstrated how to generate targeted adversarial examples using our classification model resnet 2. We iteratively applied perturbations to the input images to misclassify them into a specified target class. By visualizing the original images, perturbations, and adversarial images, it is possible to see the impact of the adversarial attack. Even though the changes introduced by the perturbations are not visible to the human eye, they are sufficient to fool the classification model into misclassifying the images into the target class. "]},{"cell_type":"markdown","metadata":{},"source":["## 4.2 Fooling the Classifier\n","### 4.2.1. Load Classification Model & Freeze weigths \n","Freeze weights because we want to fool this specific model, not some variant"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["model_weights_path = os.path.join(weights_folder, \"RESNET.weights.h5\")\n","\n","# Load the model and weights for further use\n","resnet_model = compile_resnet_model()\n","resnet_model.load_weights(model_weights_path)\n"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["# Freeze the weights\n","for layer in resnet_model.layers:\n","    layer.trainable = False\n","classification_model = resnet_model"]},{"cell_type":"markdown","metadata":{},"source":["### 4.2.2. Define Your Task\n","\n","The classes to be fooled will not be chosen randomly. We need to select classes that already exhibit good predictive performance by the classifier. The rationale behind this is that if the classifier does not initially perform well on these classes, it becomes challenging to discern whether we are truly fooling the classifier or if it is merely making incorrect predictions as it has done previously.\n","\n","Thus, we will look at classification metrics for all classes."]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["# Assuming X_train and y_train are already defined\n","\n","# Calculate the split index for validation set\n","val_split_index = int(0.8 * len(X_train))\n","\n","# Create the validation set from the training data\n","X_val = X_train[val_split_index:]\n","y_val = y_train[val_split_index:]\n","\n","print(\"Validation set:\", X_val.shape, y_val.shape)"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["from sklearn.preprocessing import LabelEncoder\n","\n","# Assuming you have a list of class names\n","class_names = ['aeroplane', 'bicycle', 'bird', 'boat', 'bottle', \n","               'bus', 'car', 'cat', 'chair', 'cow', 'diningtable', \n","               'dog', 'horse', 'motorbike', 'person', 'pottedplant', \n","               'sheep', 'sofa', 'train', 'tvmonitor']\n","\n","# Create and fit the LabelEncoder\n","encoder = LabelEncoder()\n","encoder.fit(class_names)"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["from sklearn.metrics import classification_report, average_precision_score, accuracy_score\n","# Assuming X_train, y_train, X_val, y_val, X_test, y_test are defined and preprocessed\n","\n","# Function to get classification metrics\n","def print_classification_metrics(model, X_test, y_test, encoder):\n","    preds = np.round(model.predict(X_test), 0)\n","    classification_metrics = classification_report(y_test, preds, target_names=encoder.classes_)\n","    print(classification_metrics)\n","    print(\"mAP: {:.2f}%\".format(average_precision_score(y_test, preds) * 100))\n","\n","# Display the classification metrics for the validation set\n","print_classification_metrics(resnet_model, X_val, y_val, encoder)"]},{"cell_type":"markdown","metadata":{},"source":["Let's add noise to make our classifier classify **aeroplane** as **cats**"]},{"cell_type":"markdown","metadata":{},"source":["### 4.2.3. Create the Adversarial Model\n","\n","Now, we define the adversarial model using the UNet architecture."]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["import segmentation_models as sm\n","# Set the framework for segmentation_models to use tensorflow.keras\n","sm.set_framework('tf.keras')\n","sm.framework()\n","\n","# Define and compile the adversarial model\n","img_size = 224  # Assuming the image size is 224x224\n","adversarial_model = sm.Unet('mobilenetv2', classes=3, encoder_weights='imagenet', activation='tanh', input_shape=(img_size, img_size, 3))\n","adversarial_model.summary()"]},{"cell_type":"markdown","metadata":{},"source":["### 4.2.4 Combine the Adversarial Model and Classifier"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["inputs = Input(shape=(img_size, img_size, 3))\n","delta = adversarial_model(inputs)\n","delta = ActivityRegularization(l2=1e-3)(delta)\n","input_plus_delta = Add()([inputs, delta])\n","outputs = classification_model(input_plus_delta)\n","combined_model = tf.keras.Model(inputs=inputs, outputs=outputs)\n","\n","# Compile the combined model\n","combined_model.compile(loss='binary_crossentropy', optimizer=Adam(), \n","                       metrics=['accuracy', tf.keras.metrics.Precision(), tf.keras.metrics.Recall()])\n","combined_model.summary()"]},{"cell_type":"markdown","metadata":{},"source":["### 4.2.5. Train the Combined Model"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["# Function to get index of a class object\n","def get_index_of_object(class_name):\n","    return class_names.index(class_name)"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["# Define indices for the target and deceptive classes\n","target_class = \"aeroplane\"\n","deceptive_class = \"cat\"\n","target_class_index = get_index_of_object(target_class)\n","deceptive_class_index = get_index_of_object(deceptive_class)\n","\n","# Extract images of the target class from the training set\n","X_train_target = np.array([X_train[i] for i in range(len(y_train)) if y_train[i][target_class_index] == 1])\n","y_train_target = np.array([y_train[i] for i in range(len(y_train)) if y_train[i][target_class_index] == 1])\n","\n","# Modify the labels to deceive the classifier\n","y_train_target_deceptive = y_train_target.copy()\n","for i in range(len(y_train_target_deceptive)):\n","    y_train_target_deceptive[i][target_class_index] = 0\n","    y_train_target_deceptive[i][deceptive_class_index] = 1\n","\n","# Train the combined model\n","batch_size = 32\n","epochs = 1000\n","\n","combined_model.fit(X_train_target, y_train_target_deceptive, batch_size=batch_size, epochs=epochs,\n","                   callbacks=[EarlyStopping(monitor='loss', patience=epochs//10)], verbose=1)"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["# Save the model weights\n","model_weights_path = os.path.join(weights_folder, \"adversarial_mobilenetv2.h5\")\n","adversarial_model.save_weights(model_weights_path)\n","\n","model_weights_path = os.path.join(weights_folder, \"combined_model.h5\")\n","combined_model.save_weights(model_weights_path)"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["# Load the model and weights for further use\n","#TO DO"]},{"cell_type":"markdown","metadata":{},"source":["### 4.2.6. Evaluate the Adversarial Model\n","\n","Evaluate the model's performance and visualize the adversarial examples."]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["# Assuming combined_model and adversarial_model are defined and compiled\n","\n","\n","# Function to calculate per-class accuracy\n","def per_class_accuracy(y_true, y_pred, classes):\n","    accuracies = {}\n","    for i, class_name in enumerate(classes):\n","        class_true = y_true[:, i]\n","        class_pred = y_pred[:, i]\n","        class_acc = accuracy_score(class_true, class_pred)\n","        accuracies[class_name] = class_acc\n","    return accuracies\n","\n","# Function to add adversarial noise\n","def add_adversarial_noise(adversarial_model, x):\n","    delta = adversarial_model.predict(np.array([x]))[0]\n","    x_plus_delta = x + delta\n","    return x_plus_delta, delta\n","\n","# Evaluate the impact of adversarial noise on the validation set\n","def evaluate_adversarial_noise(classification_model, adversarial_model, X_val, y_val, encoder):\n","    preds_clean = np.round(classification_model.predict(X_val), 0)\n","    preds_noisy = []\n","\n","    X_val_noisy = []\n","    for x in X_val:\n","        x_plus_delta, _ = add_adversarial_noise(adversarial_model, x)\n","        X_val_noisy.append(x_plus_delta)\n","    X_val_noisy = np.array(X_val_noisy)\n","    preds_noisy = np.round(classification_model.predict(X_val_noisy), 0)\n","\n","    # Convert numerical labels to class names for clean and noisy predictions\n","    y_val_labels = encoder.inverse_transform(np.argmax(y_val, axis=1))\n","    preds_clean_labels = encoder.inverse_transform(np.argmax(preds_clean, axis=1))\n","    preds_noisy_labels = encoder.inverse_transform(np.argmax(preds_noisy, axis=1))\n","\n","    # Compute classification metrics for clean and noisy data\n","    classification_metrics_clean = classification_report(y_val_labels, preds_clean_labels, target_names=encoder.classes_)\n","    classification_metrics_noisy = classification_report(y_val_labels, preds_noisy_labels, target_names=encoder.classes_)\n","\n","    # Compute mean Average Precision (mAP) for clean and noisy data\n","    mAP_clean = average_precision_score(y_val, preds_clean)\n","    mAP_noisy = average_precision_score(y_val, preds_noisy)\n","\n","    # Compute per-class accuracy for clean and noisy data\n","    accuracies_clean = per_class_accuracy(y_val, preds_clean, encoder.classes_)\n","    accuracies_noisy = per_class_accuracy(y_val, preds_noisy, encoder.classes_)\n","\n","    print(\"Classification metrics for clean validation set:\")\n","    print(classification_metrics_clean)\n","    print(\"mAP (clean): {:.2f}%\".format(mAP_clean * 100))\n","    print(\"\\nPer-Class Accuracy (clean):\")\n","    for class_name, acc in accuracies_clean.items():\n","        print(f\"{class_name}: {acc * 100:.2f}%\")\n","\n","    print(\"\\nClassification metrics for noisy validation set:\")\n","    print(classification_metrics_noisy)\n","    print(\"mAP (noisy): {:.2f}%\".format(mAP_noisy * 100))\n","    print(\"\\nPer-Class Accuracy (noisy):\")\n","    for class_name, acc in accuracies_noisy.items():\n","        print(f\"{class_name}: {acc * 100:.2f}%\")\n","\n","# Display the classification metrics for the validation set with and without adversarial noise\n","evaluate_adversarial_noise(classification_model, adversarial_model, X_val, y_val, encoder)\n","\n","# Visualize a few examples of the adversarial noise on the validation set\n","fig = plt.figure(figsize=(10, 10))\n","for i in range(5):\n","    x = X_val[i]\n","    x_plus_delta, delta_val = add_adversarial_noise(adversarial_model, x)\n","\n","    fig.add_subplot(5, 3, i*3+1)\n","    plt.imshow(x.astype(np.uint8))\n","    plt.title(\"Input image\")\n","    plt.axis('off')\n","\n","    fig.add_subplot(5, 3, i*3+2)\n","    plt.imshow(delta_val)\n","    plt.title(\"Adversarial noise\")\n","    plt.axis('off')\n","\n","    fig.add_subplot(5, 3, i*3+3)\n","    plt.imshow(x_plus_delta.astype(np.uint8))\n","    plt.title(\"Input for the classifier\")\n","    plt.axis('off')\n","\n","    y_pred_clean = classification_model.predict(np.array([x]))\n","    y_pred_noise = classification_model.predict(np.array([x_plus_delta]))\n","    print(f\"Prediction without noise: {y_pred_clean}, with noise: {y_pred_noise}\")"]},{"cell_type":"markdown","metadata":{},"source":["# 5. Discussion\n","## Classification\n","Multiple models were developed for the multilabel classification task. The best model was selected based on the validation accuracy. Early stopping was used to prevent overfitting of the models during training. Each model used the binary cross entropy loss function in combination with the Adam optimizer. The number of layers and type of layers and thus number of parameters was therefore different for each classification model.\n","\n","In terms of preprocessing, each image was rescaled to 224 x 224 and data augmentation was applied to enlarge the training dataset by rotating, mirroring and changing brightness of existing images and adding them to the training set.\n","\n","The classification model built from scratch had low validation accuracy and started overfitting quite quickly. Also, the model based on the Alexnet architecture did not perform well enough on the training data. However, we did notice that batch normalisation accelerated training and increased accuracy.\n","\n","Eventually, transfer learning was attempted. As expected, transfer learning using the weights gave the RESNET-50 model the best result in terms of validation accuracy. However, some more layers were added to the model. By tuning the cut-off value, a good classification performance was eventually achieved. It can thus be concluded that transfer learning works best, but still needs to be adapted to the specific context of the task.\n","\n","Possible improvements could include training the model on a more extensive dataset, or adding combinations of additional layers on top of the Resnet architecture. Futhermore, it could be interesting to test other model architectures for transfer learning such as GoogLeNet or MobileNetV2. Other possible improvements could be found by carefull hyperparameter tuning or by developing an ensemble model that combines the predictions of multiple models. \n","\n","Given its high validation accuracy of around 90%, the model could be used for real-life applications requiring multilabel classification, such as autonomous driving or classification of medical images. Despite achieving high validation accuracy during training, the score in the Kaggle competition remained rather low.\n","\n","## Semantic segmentation\n","\n","## Adversarial attacks\n"]},{"cell_type":"markdown","metadata":{},"source":["# Discussion\n","\n","## Preprocessing\n","**Resize Images to Uniform Shape**: To ensure that all images in the dataset are of the same dimensions, several strategies were considered:\n","- Idea 1: Cropping\n","    - **Description**: Crop predefined shapes from the four corners and the center of the image, resulting in a 5x bigger dataset.\n","    - **Pros**: This method introduces data augmentation by generating multiple images from a single image, potentially increasing the dataset's size and variability.\n","    - **Cons**: Cropping can result in images that do not contain all the objects of the original image, leading to inaccuracies in labeling.\n","\n","- Idea 2: Resizing\n","    - **Description**: Simply resize the images to a predefined shape, such as (224, 224).\n","    - **Pros**: Ensures uniformity across the dataset, avoiding mislabeling associated with cropping.\n","    - **Cons**: Direct resizing might distort the aspect ratio of the images.\n","\n","- Idea 3: Resizing with Padding\n","    - **Description**: Resize images while adding padding for vertical images (~20% of the training set) to maintain the aspect ratio.\n","    - **Pros**: Preserves aspect ratio and ensures uniform dimensions.\n","    - **Cons**: Padding may introduce unwanted artifacts.\n","\n","**Chosen Approach**: We opted for Idea 2: resizing the images to (224, 224). This approach ensures consistency and simplicity in the preprocessing step.\n","\n","## Image Classification\n","### Imbalanced Dataset\n","\n","**Observation**: The dataset is imbalanced, with 27% of samples belonging to the 'person' class.\n","\n","**Handling Imbalanced Datasets**\n","- Idea 1: Data Augmentation\n","    - **Description**: Augment the training dataset to balance the class distribution.\n","    - **Outcome**: This method was implemented by augmenting images, particularly focusing on the 'person' class.\n","\n","- Idea 2: Evaluation Metrics\n","    - **Description**: Use metrics like the F1 score, considering both precision and recall.\n","    - **Suggested Metric**: F1 score was chosen for its balanced measure of performance across classes.\n","\n","\n","**Chosen Approach**: [TO ADD]\n","\n","### Model Development and Improvement\n","\n","**Classification Approaches**:\n","    - Building models from scratch: Very prone to overfitting and low validation accuracy.\n","    - Utilizing existing architectures: Bad performance. Managed to improve the performance (and accelerate training) with batch normalization but still lacked in performance.\n","    - Transfer learning with RESNET-50: Achieved, as excpected, the highest validation accuracy. Layers were added to the pretrained model to adapt the model to the specific context and the cut off value was tuned to improve performance. \n","\n","**Possible Improvements**: \n","- Explore other architectures like GoogLeNet or MobileNetV2 (alread tried: VGG, Inception) \n","- perform more extensive hyperparameter tuning (for example using grid search)\n","- develop an ensemble model \n","\n","Despite achieving high validation accuracy (~90%) with the RESNET model, its performance in the Kaggle competition remained subpar.\n","\n","## Image Segmentation\n","\n","[TO ADD]\n","\n","## Adversarial Attack\n","\n","### Simple Attack: Misclassification of All Images\n","Employing the Fast Gradient Sign Method (FGSM) on all images to induce misclassification.\n","\n","### Advanced Attack: Targeting Specific Classifications\n","- **Attempt 1**: Utilizing XXX - unsuccessful. \n","\n","[TO ADD: Architecture description]\n","[TO ADD: Failure analysis]\n","\n","- **Attempt 2**: Employing FGSM - successful. \n","\n","[TO ADD: Explanation of method]\n","\n","[TO ADD: Results]\n","\n","## Real-World Application and Reflection\n","\n","### Classification: \n","Given the model's high validation accuracy of around 90%, the model could be used for real-life applications requiring multilabel classification, such as autonomous driving or classification of medical images. Despite achieving high validation accuracy during training, the score in the Kaggle competition remained rather low.\n"]}],"metadata":{"kaggle":{"accelerator":"nvidiaTeslaT4","dataSources":[{"databundleVersionId":7751254,"sourceId":70925,"sourceType":"competition"}],"dockerImageVersionId":30699,"isGpuEnabled":true,"isInternetEnabled":true,"language":"python","sourceType":"notebook"},"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.10.13"}},"nbformat":4,"nbformat_minor":4}
